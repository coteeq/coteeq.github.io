<HTML>
<BODY>
<PRE>
<!-- Manpage converted by man2html 3.0.1 -->

</PRE>
<H2>SYNOPSIS</H2><PRE>
       <B>#include</B> <B>&lt;liburing.h&gt;</B>

       <B>int</B> <B>io_uring_setup(u32</B> <I>entries</I><B>,</B> <B>struct</B> <B>io_uring_params</B> <B>*</B><I>p</I><B>);</B>


</PRE>
<H2>DESCRIPTION</H2><PRE>
       The <B><A HREF="io_uring_setup.2.html">io_uring_setup(2)</A></B> system call sets up a submission queue (SQ) and
       completion queue (CQ) with at least <I>entries</I> entries, and returns a file
       descriptor which can be used to perform subsequent operations on the
       io_uring instance.  The submission and completion queues are shared
       between userspace and the kernel, which eliminates the need to copy data
       when initiating and completing I/O.

       <I>params</I> is used by the application to pass options to the kernel, and by
       the kernel to convey information about the ring buffers.

           struct io_uring_params {
               __u32 sq_entries;
               __u32 cq_entries;
               __u32 flags;
               __u32 sq_thread_cpu;
               __u32 sq_thread_idle;
               __u32 features;
               __u32 wq_fd;
               __u32 resv[3];
               struct io_sqring_offsets sq_off;
               struct io_cqring_offsets cq_off;
           };

       The <I>flags</I>, <I>sq</I><B>_</B><I>thread</I><B>_</B><I>cpu</I>, and <I>sq</I><B>_</B><I>thread</I><B>_</B><I>idle</I> fields are used to configure
       the io_uring instance.  <I>flags</I> is a bit mask of 0 or more of the following
       values ORed together:

       <B>IORING_SETUP_IOPOLL</B>
              Perform busy-waiting for an I/O completion, as opposed to getting
              notifications via an asynchronous IRQ (Interrupt Request).  The
              file system (if any) and block device must support polling in
              order for this to work.  Busy-waiting provides lower latency, but
              may consume more CPU resources than interrupt driven I/O.
              Currently, this feature is usable only on a file descriptor opened
              using the <B>O_DIRECT</B> flag.  When a read or write is submitted to a
              polled context, the application must poll for completions on the
              CQ ring by calling <B><A HREF="io_uring_enter.2.html">io_uring_enter(2)</A></B>.  It is illegal to mix and
              match polled and non-polled I/O on an io_uring instance.

              This is only applicable for storage devices for now, and the
              storage device must be configured for polling. How to do that
              depends on the device type in question. For NVMe devices, the nvme
              driver must be loaded with the <I>poll</I><B>_</B><I>queues</I> parameter set to the
              desired number of polling queues. The polling queues will be
              shared appropriately between the CPUs in the system, if the number
              is less than the number of online CPU threads.
              application must call <B><A HREF="io_uring_enter.2.html">io_uring_enter(2)</A></B> to wake the kernel thread.
              If I/O is kept busy, the kernel thread will never sleep.  An
              application making use of this feature will need to guard the
              <B><A HREF="io_uring_enter.2.html">io_uring_enter(2)</A></B> call with the following code sequence:

                  /*
                   * Ensure that the wakeup flag is read after the tail pointer
                   * has been written. It's important to use memory load acquire
                   * semantics for the flags read, as otherwise the application
                   * and the kernel might not agree on the consistency of the
                   * wakeup flag.
                   */
                  unsigned flags = atomic_load_relaxed(sq_ring-&gt;flags);
                  if (flags &amp; IORING_SQ_NEED_WAKEUP)
                      io_uring_enter(fd, 0, 0, IORING_ENTER_SQ_WAKEUP);

       where <I>sq</I><B>_</B><I>ring</I> is a submission queue ring setup using the <I>struct</I>
       <I>io</I><B>_</B><I>sqring</I><B>_</B><I>offsets</I> described below.

              Note that, when using a ring setup with <B>IORING_SETUP_SQPOLL,</B> you
              never directly call the <B><A HREF="io_uring_enter.2.html">io_uring_enter(2)</A></B> system call. That is
              usually taken care of by liburing's <B><A HREF="io_uring_submit.3.html">io_uring_submit(3)</A></B> function.
              It automatically determines if you are using polling mode or not
              and deals with when your program needs to call <B><A HREF="io_uring_enter.2.html">io_uring_enter(2)</A></B>
              without you having to bother about it.

              Before version 5.11 of the Linux kernel, to successfully use this
              feature, the application must register a set of files to be used
              for IO through <B><A HREF="io_uring_register.2.html">io_uring_register(2)</A></B> using the
              <B>IORING_REGISTER_FILES</B> opcode. Failure to do so will result in
              submitted IO being errored with <B>EBADF.</B>  The presence of this
              feature can be detected by the <B>IORING_FEAT_SQPOLL_NONFIXED</B> feature
              flag.  In version 5.11 and later, it is no longer necessary to
              register files to use this feature. 5.11 also allows using this as
              non-root, if the user has the <B>CAP_SYS_NICE</B> capability. In 5.13
              this requirement was also relaxed, and no special privileges are
              needed for SQPOLL in newer kernels. Certain stable kernels older
              than 5.13 may also support unprivileged SQPOLL.

       <B>IORING_SETUP_SQ_AFF</B>
              If this flag is specified, then the poll thread will be bound to
              the cpu set in the <I>sq</I><B>_</B><I>thread</I><B>_</B><I>cpu</I> field of the <I>struct</I>
              <I>io</I><B>_</B><I>uring</I><B>_</B><I>params</I>.  This flag is only meaningful when
              <B>IORING_SETUP_SQPOLL</B> is specified. When cgroup setting <I>cpuset.cpus</I>
              changes (typically in container environment), the bounded cpu set
              may be changed as well.

       <B>IORING_SETUP_CQSIZE</B>
              Create the completion queue with <I>struct</I> <I>io</I><B>_</B><I>uring</I><B>_</B><I>params.cq</I><B>_</B><I>entries</I>
              entries.  The value must be greater than <I>entries</I>, and may be
              rounded up to the next power-of-two.


       <B>IORING_SETUP_R_DISABLED</B>
              If this flag is specified, the io_uring ring starts in a disabled
              state.  In this state, restrictions can be registered, but
              submissions are not allowed.  See <B><A HREF="io_uring_register.2.html">io_uring_register(2)</A></B> for details
              on how to enable the ring. Available since 5.10.

       <B>IORING_SETUP_SUBMIT_ALL</B>
              Normally io_uring stops submitting a batch of request, if one of
              these requests results in an error. This can cause submission of
              less than what is expected, if a request ends in error while being
              submitted. If the ring is created with this flag,
              <B><A HREF="io_uring_enter.2.html">io_uring_enter(2)</A></B> will continue submitting requests even if it
              encounters an error submitting a request. CQEs are still posted
              for errored request regardless of whether or not this flag is set
              at ring creation time, the only difference is if the submit
              sequence is halted or continued when an error is observed.
              Available since 5.18.

       <B>IORING_SETUP_COOP_TASKRUN</B>
              By default, io_uring will interrupt a task running in userspace
              when a completion event comes in. This is to ensure that
              completions run in a timely manner. For a lot of use cases, this
              is overkill and can cause reduced performance from both the inter-
              processor interrupt used to do this, the kernel/user transition,
              the needless interruption of the tasks userspace activities, and
              reduced batching if completions come in at a rapid rate. Most
              applications don't need the forceful interruption, as the events
              are processed at any kernel/user transition. The exception are
              setups where the application uses multiple threads operating on
              the same ring, where the application waiting on completions isn't
              the one that submitted them. For most other use cases, setting
              this flag will improve performance. Available since 5.19.

       <B>IORING_SETUP_TASKRUN_FLAG</B>
              Used in conjunction with <B>IORING_SETUP_COOP_TASKRUN,</B> this provides
              a flag, <B>IORING_SQ_TASKRUN,</B> which is set in the SQ ring <I>flags</I>
              whenever completions are pending that should be processed.
              liburing will check for this flag even when doing
              <B><A HREF="io_uring_peek_cqe.3.html">io_uring_peek_cqe(3)</A></B> and enter the kernel to process them, and
              applications can do the same. This makes <B>IORING_SETUP_TASKRUN_FLAG</B>
              safe to use even when applications rely on a peek style operation
              on the CQ ring to see if anything might be pending to reap.
              Available since 5.19.

       <B>IORING_SETUP_SQE128</B>
              If set, io_uring will use 128-byte SQEs rather than the normal
              64-byte sized variant. This is a requirement for using certain
              request types, as of 5.19 only the <B>IORING_OP_URING_CMD</B> passthrough
              command for NVMe passthrough needs this. Available since 5.19.

       <B>IORING_SETUP_CQE32</B>
              submissions on behalf of the userspace and so it always complies
              with the rule disregarding how many userspace tasks do
              <B>io_uring_enter(2).</B>  Available since 6.0.

       <B>IORING_SETUP_DEFER_TASKRUN</B>
              By default, io_uring will process all outstanding work at the end
              of any system call or thread interrupt. This can delay the
              application from making other progress.  Setting this flag will
              hint to io_uring that it should defer work until an
              <B>io_uring_enter(2)</B> call with the <B>IORING_ENTER_GETEVENTS</B> flag set.
              This allows the application to request work to run just before it
              wants to process completions.  This flag requires the
              <B>IORING_SETUP_SINGLE_ISSUER</B> flag to be set, and also enforces that
              the call to <B>io_uring_enter(2)</B> is called from the same thread that
              submitted requests.  Note that if this flag is set then it is the
              application's responsibility to periodically trigger work (for
              example via any of the CQE waiting functions) or else completions
              may not be delivered.  Available since 6.1.

       If no flags are specified, the io_uring instance is setup for interrupt
       driven I/O.  I/O may be submitted using <B><A HREF="io_uring_enter.2.html">io_uring_enter(2)</A></B> and can be
       reaped by polling the completion queue.

       The <I>resv</I> array must be initialized to zero.

       <I>features</I> is filled in by the kernel, which specifies various features
       supported by current kernel version.

       <B>IORING_FEAT_SINGLE_MMAP</B>
              If this flag is set, the two SQ and CQ rings can be mapped with a
              single <I>mmap(2)</I> call. The SQEs must still be allocated separately.
              This brings the necessary <I>mmap(2)</I> calls down from three to two.
              Available since kernel 5.4.

       <B>IORING_FEAT_NODROP</B>
              If this flag is set, io_uring supports almost never dropping
              completion events.  If a completion event occurs and the CQ ring
              is full, the kernel stores the event internally until such a time
              that the CQ ring has room for more entries. If this overflow
              condition is entered, attempting to submit more IO will fail with
              the <B>-EBUSY</B> error value, if it can't flush the overflown events to
              the CQ ring. If this happens, the application must reap events
              from the CQ ring and attempt the submit again. If the kernel has
              no free memory to store the event internally it will be visible by
              an increase in the overflow value on the cqring.  Available since
              kernel 5.5. Additionally <B><A HREF="io_uring_enter.2.html">io_uring_enter(2)</A></B> will return <B>-EBADR</B> the
              next time it would otherwise sleep waiting for completions (since
              kernel 5.19).


       <B>IORING_FEAT_SUBMIT_STABLE</B>
              If this flag is set, applications can be certain that any data for

       <B>IORING_FEAT_CUR_PERSONALITY</B>
              If this flag is set, then io_uring guarantees that both sync and
              async execution of a request assumes the credentials of the task
              that called <I>io</I><B>_</B><I>uring</I><B>_</B><I>enter(2)</I> to queue the requests. If this flag
              isn't set, then requests are issued with the credentials of the
              task that originally registered the io_uring. If only one task is
              using a ring, then this flag doesn't matter as the credentials
              will always be the same. Note that this is the default behavior,
              tasks can still register different personalities through
              <I>io</I><B>_</B><I>uring</I><B>_</B><I>register(2)</I> with <B>IORING_REGISTER_PERSONALITY</B> and specify
              the personality to use in the sqe. Available since kernel 5.6.

       <B>IORING_FEAT_FAST_POLL</B>
              If this flag is set, then io_uring supports using an internal poll
              mechanism to drive data/space readiness. This means that requests
              that cannot read or write data to a file no longer need to be
              punted to an async thread for handling, instead they will begin
              operation when the file is ready. This is similar to doing poll +
              read/write in userspace, but eliminates the need to do so. If this
              flag is set, requests waiting on space/data consume a lot less
              resources doing so as they are not blocking a thread. Available
              since kernel 5.7.

       <B>IORING_FEAT_POLL_32BITS</B>
              If this flag is set, the <B>IORING_OP_POLL_ADD</B> command accepts the
              full 32-bit range of epoll based flags. Most notably
              <B>EPOLLEXCLUSIVE</B> which allows exclusive (waking single waiters)
              behavior. Available since kernel 5.9.

       <B>IORING_FEAT_SQPOLL_NONFIXED</B>
              If this flag is set, the <B>IORING_SETUP_SQPOLL</B> feature no longer
              requires the use of fixed files. Any normal file descriptor can be
              used for IO commands without needing registration. Available since
              kernel 5.11.

       <B>IORING_FEAT_ENTER_EXT_ARG</B>
              If this flag is set, then the <B><A HREF="io_uring_enter.2.html">io_uring_enter(2)</A></B> system call
              supports passing in an extended argument instead of just the
              <I>sigset</I><B>_</B><I>t</I> of earlier kernels. This.  extended argument is of type
              <I>struct</I> <I>io</I><B>_</B><I>uring</I><B>_</B><I>getevents</I><B>_</B><I>arg</I> and allows the caller to pass in
              both a <I>sigset</I><B>_</B><I>t</I> and a timeout argument for waiting on events. The
              struct layout is as follows:
       struct io_uring_getevents_arg {
           __u64 sigmask;
           __u32 sigmask_sz;
           __u32 pad;
           __u64 ts;
       };

       and a pointer to this struct must be passed in if <B>IORING_ENTER_EXT_ARG</B> is
       set in the flags for the enter system call. Available since kernel 5.11.

       <B>IORING_FEAT_CQE_SKIP</B>
              If this flag is set, then io_uring supports setting
              <B>IOSQE_CQE_SKIP_SUCCESS</B> in the submitted SQE, indicating that no
              CQE should be generated for this SQE if it executes normally. If
              an error happens processing the SQE, a CQE with the appropriate
              error value will still be generated. Available since kernel 5.17.

       <B>IORING_FEAT_LINKED_FILE</B>
              If this flag is set, then io_uring supports sane assignment of
              files for SQEs that have dependencies. For example, if a chain of
              SQEs are submitted with <B>IOSQE_IO_LINK,</B> then kernels without this
              flag will prepare the file for each link upfront.  If a previous
              link opens a file with a known index, eg if direct descriptors are
              used with open or accept, then file assignment needs to happen
              post execution of that SQE. If this flag is set, then the kernel
              will defer file assignment until execution of a given request is
              started. Available since kernel 5.17.

       <B>IORING_FEAT_REG_REG_RING</B>
              If this flag is set, then io_uring supports calling
              <B><A HREF="io_uring_register.2.html">io_uring_register(2)</A></B> using a registered ring fd, via
              <B>IORING_REGISTER_USE_REGISTERED_RING</B>.  Available since kernel 6.3.


       The rest of the fields in the <I>struct</I> <I>io</I><B>_</B><I>uring</I><B>_</B><I>params</I> are filled in by the
       kernel, and provide the information necessary to memory map the
       submission queue, completion queue, and the array of submission queue
       entries.  <I>sq</I><B>_</B><I>entries</I> specifies the number of submission queue entries
       allocated.  <I>sq</I><B>_</B><I>off</I> describes the offsets of various ring buffer fields:

           struct io_sqring_offsets {
               __u32 head;
               __u32 tail;
               __u32 ring_mask;
               __u32 ring_entries;
               __u32 flags;
               __u32 dropped;
               __u32 array;
               __u32 resv[3];
           };

       Taken together, <I>sq</I><B>_</B><I>entries</I> and <I>sq</I><B>_</B><I>off</I> provide all of the information
       necessary for accessing the submission queue ring buffer and the
       submission queue entry array.  The submission queue can be mapped with a
       call like:

           ptr = mmap(0, sq_off.array + sq_entries * sizeof(__u32),
                      PROT_READ|PROT_WRITE, MAP_SHARED|MAP_POPULATE,
                      ring_fd, IORING_OFF_SQ_RING);

       where <I>sq</I><B>_</B><I>off</I> is the <I>io</I><B>_</B><I>sqring</I><B>_</B><I>offsets</I> structure, and <I>ring</I><B>_</B><I>fd</I> is the file
       descriptor returned from <B><A HREF="io_uring_setup.2.html">io_uring_setup(2)</A></B>.  The addition of <I>sq</I><B>_</B><I>off.array</I>
       by the application when submitting new I/O, and the head is incremented
       by the kernel when the I/O has been successfully submitted.  Determining
       the index of the head or tail into the ring is accomplished by applying a
       mask:

           index = tail &amp; ring_mask;

       The array of submission queue entries is mapped with:

           sqentries = mmap(0, sq_entries * sizeof(struct io_uring_sqe),
                            PROT_READ|PROT_WRITE, MAP_SHARED|MAP_POPULATE,
                            ring_fd, IORING_OFF_SQES);

       The completion queue is described by <I>cq</I><B>_</B><I>entries</I> and <I>cq</I><B>_</B><I>off</I> shown here:

           struct io_cqring_offsets {
               __u32 head;
               __u32 tail;
               __u32 ring_mask;
               __u32 ring_entries;
               __u32 overflow;
               __u32 cqes;
               __u32 flags;
               __u32 resv[3];
           };

       The completion queue is simpler, since the entries are not separated from
       the queue itself, and can be mapped with:

           ptr = mmap(0, cq_off.cqes + cq_entries * sizeof(struct io_uring_cqe),
                      PROT_READ|PROT_WRITE, MAP_SHARED|MAP_POPULATE, ring_fd,
                      IORING_OFF_CQ_RING);

       Closing the file descriptor returned by <B><A HREF="io_uring_setup.2.html">io_uring_setup(2)</A></B> will free all
       resources associated with the io_uring context. Note that this may happen
       asynchronously within the kernel, so it is not guaranteed that resources
       are freed immediately.


</PRE>
<H2>RETURN VALUE</H2><PRE>
       <B><A HREF="io_uring_setup.2.html">io_uring_setup(2)</A></B> returns a new file descriptor on success.  The
       application may then provide the file descriptor in a subsequent <B><A HREF="mmap.2.html">mmap(2)</A></B>
       call to map the submission and completion queues, or to the
       <B><A HREF="io_uring_register.2.html">io_uring_register(2)</A></B> or <B><A HREF="io_uring_enter.2.html">io_uring_enter(2)</A></B> system calls.

       On error, a negative error code is returned. The caller should not rely
       on <I>errno</I> variable.


</PRE>
<H2>ERRORS</H2><PRE>
       <B>EFAULT</B> params is outside your accessible address space.

       <B>EINVAL</B> The resv array contains non-zero data, p.flags contains an
              unsupported flag, <I>entries</I> is out of bounds, <B>IORING_SETUP_SQ_AFF</B>
              the caller did not have sufficient privileges.


</PRE>
<H2>SEE ALSO</H2><PRE>
       <B><A HREF="io_uring_register.2.html">io_uring_register(2)</A></B>, <B><A HREF="io_uring_enter.2.html">io_uring_enter(2)</A></B>



Linux                              2019-01-29                  <B><A HREF="io_uring_setup.2.html">io_uring_setup(2)</A></B>
</PRE>
<HR>
<ADDRESS>
Man(1) output converted with
<a href="http://www.oac.uci.edu/indiv/ehood/man2html.html">man2html</a>
</ADDRESS>
</BODY>
</HTML>
